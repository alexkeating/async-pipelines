{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Async-pipelines is a lightweight framework that allows a developer to apply a consumer producer pattern to their code. Some of the main benefits of this pattern is indepedent scaling of differeny IO actions and easy ability to break out indivdual producers and consumers into seperate jobs. It is currently in development and gives the user the following: SNS support SQS support Postgres Support Requirements 3.7+ pip install async-pipelines Example example.py import random from typing import Optional from pipelines import Job , Queue , MultiQueue async def randsleep ( a : int = 1 , b : int = 5 , caller = None ) -> None : i = random . randint ( 0 , 10 ) if caller : print ( f \" {caller} sleeping for {i} seconds.\" ) await asyncio . sleep ( i ) class ExampleProducer ( Job ): async def start ( self , in_q : Optional [ Queue ], out_q : Optional [ MultiQueue ]) -> None : for x in range ( 1 , 10 ): await randsleep ( caller = f \"Producer\" ) print ( f \"Producer {x} \" ) await out_q . put ( x ) class ExampleProducerConsumer ( Job ): async def start ( self , in_q : Optional [ asyncio . Queue ], out_q : Optional [ MultiQueue ] ) -> None : while True : num = await in_q . get () print ( f \"Consumer/Producer {num} \" ) await out_q . put ( num ) in_q . task_done () class FullConsumer ( Job ): workers = 10 def __init__ ( self , name : str ) -> None : super () . __init__ () self . name = name async def start ( self , in_q : Optional [ asyncio . Queue ], out_q : Optional [ MultiQueue ] ) -> None : while True : num = await in_q . get () print ( f \" {self.name} {num} \" ) in_q . task_done () async def main () -> None : # Different producers and consumers root = ExampleProducer () producer_consumer = ExampleProducerConsumer () consumer = FullConsumer ( name = \"Full Consumer\" ) consumer2 = FullConsumer ( name = \"Partial Person Ser\" ) consumer3 = FullConsumer ( name = \"Second One to consume\" ) # Subscribing a consumer to a producer root . set_downstream ( producer_consumer ) root . set_downstream ( consumer2 ) producer_consumer . set_downstream ( consumer ) producer_consumer . set_downstream ( consumer3 ) await root . execute_jobs () asyncio . run ( main ()) Modularity Async-pipelines is meant to be modular, composable and extensible. Each job within a pipeline should inherit from the jobs base class.","title":"Home"},{"location":"#introduction","text":"Async-pipelines is a lightweight framework that allows a developer to apply a consumer producer pattern to their code. Some of the main benefits of this pattern is indepedent scaling of differeny IO actions and easy ability to break out indivdual producers and consumers into seperate jobs. It is currently in development and gives the user the following: SNS support SQS support Postgres Support","title":"Introduction"},{"location":"#requirements-37","text":"pip install async-pipelines","title":"Requirements 3.7+"},{"location":"#example","text":"example.py import random from typing import Optional from pipelines import Job , Queue , MultiQueue async def randsleep ( a : int = 1 , b : int = 5 , caller = None ) -> None : i = random . randint ( 0 , 10 ) if caller : print ( f \" {caller} sleeping for {i} seconds.\" ) await asyncio . sleep ( i ) class ExampleProducer ( Job ): async def start ( self , in_q : Optional [ Queue ], out_q : Optional [ MultiQueue ]) -> None : for x in range ( 1 , 10 ): await randsleep ( caller = f \"Producer\" ) print ( f \"Producer {x} \" ) await out_q . put ( x ) class ExampleProducerConsumer ( Job ): async def start ( self , in_q : Optional [ asyncio . Queue ], out_q : Optional [ MultiQueue ] ) -> None : while True : num = await in_q . get () print ( f \"Consumer/Producer {num} \" ) await out_q . put ( num ) in_q . task_done () class FullConsumer ( Job ): workers = 10 def __init__ ( self , name : str ) -> None : super () . __init__ () self . name = name async def start ( self , in_q : Optional [ asyncio . Queue ], out_q : Optional [ MultiQueue ] ) -> None : while True : num = await in_q . get () print ( f \" {self.name} {num} \" ) in_q . task_done () async def main () -> None : # Different producers and consumers root = ExampleProducer () producer_consumer = ExampleProducerConsumer () consumer = FullConsumer ( name = \"Full Consumer\" ) consumer2 = FullConsumer ( name = \"Partial Person Ser\" ) consumer3 = FullConsumer ( name = \"Second One to consume\" ) # Subscribing a consumer to a producer root . set_downstream ( producer_consumer ) root . set_downstream ( consumer2 ) producer_consumer . set_downstream ( consumer ) producer_consumer . set_downstream ( consumer3 ) await root . execute_jobs () asyncio . run ( main ())","title":"Example"},{"location":"#modularity","text":"Async-pipelines is meant to be modular, composable and extensible. Each job within a pipeline should inherit from the jobs base class.","title":"Modularity"},{"location":"api/","text":"Full library api Base Data Structures class pipelines. Job ( ) A base class for all jobs in a pipeline. It requires the start method to be implmeneted and has helper utilties to add children jobs and execute a pipeline. Parameters queue : Queue used to send messages to another job workers : Number of Tasks created for this job children : Jobs which receive messages from this Job start ( self , in_q , out_q ) Method executed when job is run. Typically this will not need to be called directly, and will instead be called within execute_jobs. This method should mark every task from in_q as done. Parameters in_q - The queue to receive messages from to process out_q - The queue to send messages to be processed execute_jobs ( self ) Method used to execute a pipeline. This should be called on the root node of the pipeline. set_downstream ( self , job ) Add child job Parameters job - Job to added as a child class pipelines. Pipe ( parent , queue , subscribed_queues ) A class that holds the data for what queues a job should read from and what queues a job should write to. Parameters : parent - Job to be hooked into pipeline queue - Queue Job is going to get messages from subscribed_queues - Queue messages will be sent too class pipelines. MultiQueue ( queues=None ) A class that allows a user to encapsulate multiple queues and push messages to those suscribed queues. Parameters : queues - subscribed queues put ( self , item ) Pushes message to queues Parameters : item - A piece of data to send to all subscribed queues add_queue ( self , q ) Add queue to subscribed queues Parameters : q : A asyncio queue","title":"Api"},{"location":"api/#full-library-api","text":"","title":"Full library api"},{"location":"api/#base-data-structures","text":"class pipelines. Job ( ) A base class for all jobs in a pipeline. It requires the start method to be implmeneted and has helper utilties to add children jobs and execute a pipeline. Parameters queue : Queue used to send messages to another job workers : Number of Tasks created for this job children : Jobs which receive messages from this Job start ( self , in_q , out_q ) Method executed when job is run. Typically this will not need to be called directly, and will instead be called within execute_jobs. This method should mark every task from in_q as done. Parameters in_q - The queue to receive messages from to process out_q - The queue to send messages to be processed execute_jobs ( self ) Method used to execute a pipeline. This should be called on the root node of the pipeline. set_downstream ( self , job ) Add child job Parameters job - Job to added as a child class pipelines. Pipe ( parent , queue , subscribed_queues ) A class that holds the data for what queues a job should read from and what queues a job should write to. Parameters : parent - Job to be hooked into pipeline queue - Queue Job is going to get messages from subscribed_queues - Queue messages will be sent too class pipelines. MultiQueue ( queues=None ) A class that allows a user to encapsulate multiple queues and push messages to those suscribed queues. Parameters : queues - subscribed queues put ( self , item ) Pushes message to queues Parameters : item - A piece of data to send to all subscribed queues add_queue ( self , q ) Add queue to subscribed queues Parameters : q : A asyncio queue","title":"Base Data Structures"}]}